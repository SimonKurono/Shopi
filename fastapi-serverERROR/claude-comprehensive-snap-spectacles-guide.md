# Complete Snap Spectacles Development Guide
### Generated by Claude for Building AR Experiences with Lens Studio

---

## Table of Contents

1. [Introduction & Overview](#introduction--overview)
2. [Prerequisites & Environment Setup](#prerequisites--environment-setup)
3. [Getting Started with Lens Studio](#getting-started-with-lens-studio)
4. [Spectacles Interaction Kit (SIK) Deep Dive](#spectacles-interaction-kit-sik-deep-dive)
5. [Project Architecture & Best Practices](#project-architecture--best-practices)
6. [Building with TypeScript](#building-with-typescript)
7. [Core Features & Components](#core-features--components)
8. [Example Projects Breakdown](#example-projects-breakdown)
9. [Testing, Debugging & Deployment](#testing-debugging--deployment)
10. [API Reference Quick Links](#api-reference-quick-links)
11. [Troubleshooting Guide](#troubleshooting-guide)
12. [Advanced Topics & Extensions](#advanced-topics--extensions)

---

## Introduction & Overview

### What Are Snap Spectacles?

Snap Spectacles are AR glasses that enable hands-free augmented reality experiences. The latest generation (Spectacles 2024) runs on **Snap OS** and is designed for:

- **Hands-free interaction** via hand tracking and voice commands
- **Connected multiplayer experiences** with built-in backend infrastructure
- **Natural gestures** including pinching, poking, and voice-based commands
- **Wireless development** with seamless deployment from Lens Studio

### Development Platform: Lens Studio

**Lens Studio** is Snap's end-to-end AR-first development platform that supports:

- Visual scene editing and asset management
- TypeScript/JavaScript scripting
- Physics engines and prefabs
- Multi-scene support and package management
- Version control integration
- Real-time wireless deployment to Spectacles hardware

### Key Capabilities

- **Hand Tracking**: Natural hand gestures (pinch, poke, ray-based interactions)
- **Voice ML**: Voice command recognition and transcription
- **Mobile Controller**: Companion phone app as input device
- **Location Services**: GPS, compass, and Snap Places API
- **Computer Vision**: Object tracking, surface detection, body tracking
- **Multiplayer**: Shared AR experiences with built-in networking
- **Remote APIs**: Integration with external services and commerce platforms

---

## Prerequisites & Environment Setup

### Required Software

1. **Lens Studio 5.15+** (required for Spectacles 2024)
   - Download: https://ar.snap.com/download
   - Note: Lens Studio 5.15 is the last anticipated update for Spectacles (2024)

2. **Node.js 18+** (for TypeScript development outside Lens Studio)
   - Optional but recommended for advanced workflows

3. **Git with Git LFS** (for version control of binary assets)
   - Required for large textures, HDR images, meshes

4. **Graphics Tools** (optional)
   - Blender, Photoshop, or similar for creating/editing assets

### Required Accounts

- **Snap Developer Account**
  - Sign up at https://developers.snap.com
  - Required for accessing Remote Service Modules (Snap Places API, etc.)
  - Required for publishing lenses

### Testing Devices

- **Spectacles 2021 or Spectacles 3** (recommended for on-device testing)
- **iOS/Android device** running Snapchat app (for mobile lens testing)

### Capabilities & Permissions

When building for Spectacles, enable the following capabilities in Lens Studio:

- **Hand Tracking** (for gesture interactions)
- **Voice ML** (for voice commands)
- **Location Services** (for GPS/mapping features)
- **Surfaces/World Mesh** (for world anchoring)
- **Network Access** (for Remote Service Modules)

---

## Getting Started with Lens Studio

### 1. Installation & Setup

1. Download and install **Lens Studio** from https://ar.snap.com/download
2. Launch Lens Studio and sign in with your Snap account
3. Enable **Spectacles Support** in the toolbar (toggle icon)

### 2. Creating Your First Spectacles Lens

**Quick Start Method:**

1. In Lens Studio home, select **Spectacles > Base Template** under "Build with Starter Templates"
2. Your new project will be preconfigured with:
   - Spectacles Interaction Kit (SIK)
   - Essential configurations for Spectacles
   - Example scene with hand tracking

**Opening Existing Projects:**

1. Choose **File > Open Project**
2. Select the `.esproj` file from your project directory
3. Lens Studio loads all resources from the project folder

### 3. Project Structure Overview

A typical Lens Studio project includes:

```
MyProject/
├── Assets/                          # Main asset directory
│   ├── SpectaclesInteractionKit/   # SIK package (UI, interactions, utilities)
│   ├── Scripts/                     # Custom TypeScript/JavaScript files
│   ├── Prefabs/                     # Reusable scene object templates
│   ├── Materials/                   # Material definitions
│   ├── Textures/                    # Image assets
│   └── Audio/                       # Sound effects and music
├── Support/
│   └── StudioLib.d.ts              # TypeScript definitions for Lens Studio APIs
├── PluginsUserPreferences/          # Editor preferences (ignore for builds)
├── Workspaces/                      # Editor layouts (ignore for builds)
└── project.esproj                   # Lens Studio project file
```

### 4. Scene Hierarchy & Resources Panel

- **Scene Hierarchy** (left panel): Shows all SceneObjects in your scene
- **Resources Panel** (bottom): Lists all assets (scripts, textures, prefabs, etc.)
- **Inspector** (right): Shows properties of selected SceneObject or resource
- **Preview Panel** (center): Real-time preview of your lens

### 5. Connecting to Spectacles Hardware

**Pairing Steps:**

1. Turn on your Spectacles and ensure they're updated
2. In Lens Studio, go to **Spectacles > Pair Device**
3. Follow on-screen instructions to connect via Wi-Fi
4. Once paired, use **Spectacles > Send to Device** for live preview

**Testing Workflow:**

1. Make changes in Lens Studio
2. Click **Send to Device**
3. View real-time updates on Spectacles
4. Check logs in Lens Studio's **Logger panel** (View > Panels > Logger)

---

## Spectacles Interaction Kit (SIK) Deep Dive

The **Spectacles Interaction Kit (SIK)** is a collection of code components, modules, and assets that simplify development for Spectacles. It provides common building blocks for interactive AR experiences.

### SIK Architecture

```
Assets/SpectaclesInteractionKit/
├── Components/
│   └── UI/                    # UI prefabs (buttons, containers, scroll views)
│       ├── ContainerFrame/
│       ├── ScrollView/
│       ├── PinchButton/
│       └── ToggleButton/
├── Core/                      # Interaction system core
│   ├── InteractionManager/    # Central interaction coordinator
│   ├── Interactor/           # Hand interactors
│   └── Interactable/         # Objects that can be interacted with
├── Providers/                 # Data providers
│   ├── HandInputData/        # Hand tracking data
│   ├── TargetProvider/       # Target selection
│   └── CameraProvider/       # Camera information
└── Utils/                     # Utilities and helpers
    ├── Filter/
    ├── StateMachine/
    └── Views/
```

### Hand Tracking & Interactions

SIK provides three interaction modes:

#### 1. **Indirect (Ray-Based) Interactions**
- User points at objects from a distance
- Visual ray extends from hand
- Best for UI elements and distant objects

```typescript
// Access hand interactor
const interactor = SIK.InteractionManager.getTargetingInteractor();

// Check if interactor is active
if (interactor.isActive()) {
    const targetedObject = interactor.currentInteractable;
}
```

#### 2. **Direct Pinch**
- User pinches fingers together near an object
- Enables grabbing, scaling, rotating
- Natural for object manipulation

```typescript
// Detect pinch gesture
script.createEvent("OnStartEvent").bind(() => {
    const handInputData = script.getSceneObject()
        .getComponent("HandInputData");

    handInputData.onPinchDown.add((hand) => {
        print(`${hand} hand pinched!`);
    });
});
```

#### 3. **Direct Poke**
- User touches objects directly with finger
- Best for buttons and tactile interactions
- Provides haptic-like feedback

### Voice ML Integration

**Voice ML** enables voice command recognition and transcription.

#### Basic Setup

```typescript
// Import VoiceML module
const voiceML = script.getSceneObject()
    .getComponent("VoiceMLModule");

// Configure listening options
const listenOptions = new VoiceML.ListeningOptions();
listenOptions.shouldReturnAsrTranscription = true;
listenOptions.shouldReturnInterimAsrTranscription = true;

// Start listening
voiceML.startListening(listenOptions);

// Handle transcription updates
voiceML.onListeningUpdate.add((eventArgs) => {
    if (eventArgs.transcription) {
        print("Heard: " + eventArgs.transcription.trim());

        // Check if transcription is final
        if (eventArgs.isFinalTranscription) {
            handleCommand(eventArgs.transcription);
        }
    }
});

function handleCommand(text: string) {
    const command = text.toLowerCase().trim();

    if (command.includes("next")) {
        // Move to next step
    } else if (command.includes("back")) {
        // Go back
    } else if (command.includes("pause")) {
        // Pause playback
    }
}
```

### UI Components

SIK provides pre-built UI components optimized for Spectacles:

#### ContainerFrame
- Windowed UI container with clipping
- Supports animations and transitions

```typescript
// Get container frame component
const container = script.getSceneObject()
    .getComponent("ContainerFrame");

// Animate container
container.setWorldPosition(new vec3(0, 0, -100));
```

#### PinchButton
- Hand-activated button via pinch gesture
- Provides visual feedback (hover, press states)

```typescript
const pinchButton = script.getSceneObject()
    .getComponent("PinchButton");

// Subscribe to pinch event
pinchButton.onPinch.add(() => {
    print("Button pinched!");
    // Execute action
});
```

#### ScrollView with GridContentCreator
- Scrollable content container
- Dynamically populate with items

```typescript
const gridCreator = script.getSceneObject()
    .getComponent("GridContentCreator");

// Scroll to next item
gridCreator.scrollToNextInstruction();

// Scroll to previous item
gridCreator.scrollToPrevInstruction();

// Relayout after adding items
gridCreator.relayoutContent();
```

---

## Project Architecture & Best Practices

### Design Principles (from PiercePuppy's Playbook)

Following the **Zen of Python** applied to Lens development:

1. **Simple beats complex**: Start with minimal lens, layer features incrementally
2. **Explicit beats implicit**: Comment tricky logic, document public APIs
3. **Flat beats nested**: Split giant scripts; stay under 600-line sanity cap
4. **Readability counts**: Use descriptive names (`MapController`, not `Script42`)

### SOLID Principles for Lens Studio

#### Single Responsibility
- Each script should have one clear purpose
- Break up >600-line files into modules

**Bad:**
```typescript
// MapController.ts (2000 lines)
// Handles map tiles, UI, pins, API calls, animations...
```

**Good:**
```typescript
// MapController.ts - Main controller
// MapTileManager.ts - Tile loading logic
// MapPinManager.ts - Pin management
// MapUIController.ts - UI button handlers
// SnapPlacesProvider.ts - API integration
```

#### Open/Closed Principle
- Extend SIK classes instead of modifying them
- Use composition over inheritance

```typescript
// Extend existing interactable behavior
class CustomInteractable extends Interactable {
    onHoverEnter() {
        super.onHoverEnter();
        // Add custom hover effect
    }
}
```

#### Dependency Injection
- Use Lens Studio's `@input` annotations
- Treat inspector inputs like constructor parameters

```typescript
// @input SceneObject mapContainer
// @input Component.Camera mainCamera
// @input Asset.RemoteServiceModule placesAPI

// Now accessible via script.mapContainer, etc.
```

### File Organization

```
Assets/
├── Scripts/
│   ├── Core/              # Core system scripts
│   │   └── MapController.ts
│   ├── UI/                # UI controllers
│   │   ├── MapUIController.ts
│   │   └── ButtonHandler.ts
│   ├── Providers/         # Data providers
│   │   └── SnapPlacesProvider.ts
│   └── Utils/             # Utility functions
│       ├── MapUtils.ts
│       └── Logging.ts
├── Prefabs/
│   ├── UI/
│   └── Map/
├── Materials/
└── Textures/
```

### Documentation Standards

**Script Header Template:**

```typescript
/**
 * MapController.ts
 *
 * Purpose: Manages map state, tiles, and user location tracking
 *
 * Inputs:
 *   - tileCount: number - Grid size for map tiles
 *   - mapZoomLevel: number - Initial zoom level (8-21)
 *
 * Outputs:
 *   - onMapLoaded: Event - Fires when all tiles are loaded
 *   - onLocationUpdated: Event - Fires on GPS update
 *
 * Dependencies:
 *   - MapGridView.ts
 *   - LocationService (built-in)
 *
 * Side Effects:
 *   - Instantiates tile prefabs
 *   - Requests device location permission
 */
```

### Asset Management Best Practices

1. **Naming Conventions**
   - Use descriptive names: `CafePinIcon.png`, not `icon42.png`
   - Follow consistent casing: `PascalCase` for prefabs, `camelCase` for scripts

2. **Categorization**
   - Tag assets in Lens Studio (e.g., `Category: Audio/SFX`)
   - Group related assets in folders

3. **Optimization**
   - Compress large textures
   - Enable texture compression for HDR/large images
   - Use Git LFS for binary assets
   - Document memory budgets (e.g., "Keep total textures under 50MB")

4. **Prefab Encapsulation**
   - Assign scripts and references inside prefab
   - Set default materials in prefab
   - Reduces scene wiring complexity

5. **Asset Register**
   - Maintain a table listing asset name, purpose, owner, dependencies
   - Update when assets change

Example:

| Asset Name | Type | Purpose | Dependencies | Owner |
|------------|------|---------|--------------|-------|
| `MapTile.prefab` | Prefab | Map tile with texture | `MapController.ts` | Pierce |
| `PinIcon.png` | Texture | Quest marker icon | `QuestMarker.prefab` | Sarah |

---

## Building with TypeScript

### Why TypeScript?

With **Lens Studio 5.0+**, TypeScript is supported natively:

- **Type safety**: Catch errors at compile time
- **IntelliSense**: Auto-completion and inline documentation
- **Refactoring**: Rename variables/functions across files safely
- **Modern syntax**: Use classes, interfaces, enums, etc.

### TypeScript Setup

1. **Enable TypeScript in Project**
   - Go to **Project Info** in Lens Studio
   - Set scripting language to **TypeScript**

2. **Import Type Definitions**

```typescript
// Import Lens Studio API definitions
/// <reference path="../Support/StudioLib.d.ts" />

// Now you have full IntelliSense for:
// - SceneObject
// - Component
// - Transform
// - vec2, vec3, vec4, quat
// - etc.
```

### Script Inputs (`@input` annotations)

Lens Studio uses special comments to expose script properties in the Inspector:

```typescript
// @input SceneObject myObject
// @input Component.Camera mainCamera
// @input Asset.Texture icon
// @input int tileCount = 5
// @input float zoomLevel = 12.0 {"hint":"8-21"}
// @input bool enableLogging = false
// @input string userName = "Guest"
// @input vec3 startPosition = {0, 0, 0}
// @input Component.Text[] labels

// Accessible via script.<name>
print(script.tileCount); // 5
```

**Input Types:**

- Basic: `int`, `float`, `bool`, `string`
- Vectors: `vec2`, `vec3`, `vec4`, `quat`
- Assets: `Asset.Texture`, `Asset.Material`, `Asset.RemoteServiceModule`
- Components: `Component.Camera`, `Component.Text`, `Component.Image`
- Scene: `SceneObject`
- Arrays: Add `[]` after type

### Events & Script Lifecycle

```typescript
// OnStartEvent - runs once when lens starts
script.createEvent("OnStartEvent").bind(onStart);

function onStart() {
    print("Lens started!");
}

// UpdateEvent - runs every frame
script.createEvent("UpdateEvent").bind(onUpdate);

function onUpdate() {
    const deltaTime = getDeltaTime();
    // Update logic here
}

// LateUpdateEvent - runs after all updates
script.createEvent("LateUpdateEvent").bind(onLateUpdate);

function onLateUpdate() {
    // Camera follow logic, etc.
}

// OnDestroyEvent - cleanup
script.createEvent("OnDestroyEvent").bind(onDestroy);

function onDestroy() {
    // Release resources
}
```

### Working with SceneObjects

```typescript
// Get current scene object
const myObject = script.getSceneObject();

// Get transform
const transform = myObject.getTransform();

// Position
const pos = transform.getWorldPosition();
transform.setWorldPosition(new vec3(0, 10, -50));

// Rotation
const rot = transform.getWorldRotation();
transform.setWorldRotation(quat.fromEulerAngles(0, Math.PI, 0));

// Scale
const scale = transform.getWorldScale();
transform.setWorldScale(new vec3(2, 2, 2));

// Get parent
const parent = myObject.getParent();

// Get children
const childCount = myObject.getChildrenCount();
for (let i = 0; i < childCount; i++) {
    const child = myObject.getChild(i);
    print(child.name);
}

// Find child by name
function findChildByName(parent: SceneObject, name: string): SceneObject | null {
    for (let i = 0; i < parent.getChildrenCount(); i++) {
        const child = parent.getChild(i);
        if (child.name === name) return child;
    }
    return null;
}
```

### Working with Components

```typescript
// Get component
const textComponent = myObject.getComponent("Component.Text");

// Type-safe version (TypeScript)
const camera = myObject.getComponent("Component.Camera") as Camera;
camera.fov = 60 * (Math.PI / 180); // Convert degrees to radians

// Get all components of a type
const allTexts = myObject.getComponents("Component.Text");

// Add component (some types)
const newImage = myObject.createComponent("Component.Image");

// Remove component
myObject.removeComponent(textComponent);
```

### Creating Reusable Modules

**MapUtils.ts** (utility module):

```typescript
export class MapUtils {
    static degToRad(degrees: number): number {
        return degrees * (Math.PI / 180);
    }

    static calculateDistance(lat1: number, lon1: number,
                            lat2: number, lon2: number): number {
        const R = 6371000; // Earth radius in meters
        const dLat = this.degToRad(lat2 - lat1);
        const dLon = this.degToRad(lon2 - lon1);

        const a = Math.sin(dLat/2) * Math.sin(dLat/2) +
                  Math.cos(this.degToRad(lat1)) * Math.cos(this.degToRad(lat2)) *
                  Math.sin(dLon/2) * Math.sin(dLon/2);

        const c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1-a));
        return R * c;
    }

    static normalizeAngle(angle: number): number {
        while (angle > Math.PI) angle -= 2 * Math.PI;
        while (angle < -Math.PI) angle += 2 * Math.PI;
        return angle;
    }
}
```

**Using the module:**

```typescript
import { MapUtils } from "./MapUtils";

const distance = MapUtils.calculateDistance(
    34.0522, -118.2437,  // Los Angeles
    40.7128, -74.0060    // New York
);
print(`Distance: ${distance} meters`);
```

### Testing TypeScript Outside Lens Studio

For complex logic, write unit tests:

```typescript
// MapUtils.test.ts
import { MapUtils } from "./MapUtils";

describe("MapUtils", () => {
    test("degToRad converts correctly", () => {
        expect(MapUtils.degToRad(180)).toBeCloseTo(Math.PI);
        expect(MapUtils.degToRad(90)).toBeCloseTo(Math.PI / 2);
    });

    test("calculateDistance returns valid result", () => {
        const dist = MapUtils.calculateDistance(0, 0, 0, 1);
        expect(dist).toBeGreaterThan(0);
    });
});
```

Run with **Vitest** or **Jest** using mocked Lens Studio globals:

```typescript
// test-setup.ts
global.print = console.log;
global.vec2 = class { constructor(public x: number, public y: number) {} };
global.vec3 = class { constructor(public x: number, public y: number, public z: number) {} };
// ... mock other Lens Studio APIs
```

---

## Core Features & Components

### Location Services & GPS

#### Accessing Device Location

```typescript
// @input Component.DeviceLocationTrackingComponent locationTracking

script.createEvent("OnStartEvent").bind(() => {
    // Request location permission
    const locationService = script.locationTracking;

    // Listen for location updates
    locationService.onLocationUpdated.add((geoPosition) => {
        const lat = geoPosition.latitude;
        const lon = geoPosition.longitude;
        const accuracy = geoPosition.accuracy;

        print(`Location: ${lat}, ${lon} (±${accuracy}m)`);
    });

    // Get compass heading (Spectacles only)
    if (locationService.getCompassHeading) {
        const heading = locationService.getCompassHeading();
        print(`Heading: ${heading}°`);
    }
});
```

#### Snap Places API

```typescript
// SnapPlacesProvider.ts - wrapper for Remote Service Module

export class SnapPlacesProvider {
    // @input Asset.RemoteServiceModule placesAPI

    private cache: Map<string, PlaceInfo[]> = new Map();

    getNearbyPlaces(lat: number, lon: number,
                    categories?: string[]): Promise<PlaceInfo[]> {
        const cacheKey = `${lat},${lon}`;

        // Check cache first
        if (this.cache.has(cacheKey)) {
            return Promise.resolve(this.cache.get(cacheKey)!);
        }

        // Call Remote Service Module
        return new Promise((resolve, reject) => {
            const request = RemoteServiceHttpRequest.create();
            request.url = `https://places-api.snapchat.com/nearby`;
            request.method = RemoteServiceHttpRequest.HttpRequestMethod.Get;

            // Add query parameters
            request.parameters = {
                latitude: lat.toString(),
                longitude: lon.toString(),
                radius: "500", // 500 meters
            };

            // Filter by categories if provided
            if (categories && categories.length > 0) {
                request.parameters.categories = categories.join(",");
            }

            script.placesAPI.performHttpRequest(request, (response) => {
                if (response.statusCode === 200) {
                    const places = JSON.parse(response.body);
                    this.cache.set(cacheKey, places);
                    resolve(places);
                } else {
                    reject(new Error(`API error: ${response.statusCode}`));
                }
            });
        });
    }
}

interface PlaceInfo {
    name: string;
    address: string;
    latitude: number;
    longitude: number;
    category: string;
    opening_hours?: string;
}
```

### World Tracking & Surface Detection

```typescript
// worldview.ts - Surface detection and object placement

// @input SceneObject targetObject {"hint":"Object to place in world"}
// @input SceneObject hitTestReticle {"hint":"Visual indicator for hit test"}

const worldQueryModule = require("LensStudio:WorldQueryModule");
const hitTestSession = worldQueryModule.createHitTestSession();

// Get hand interactor from SIK
const interactionManager = require("SpectaclesInteractionKit/Core/InteractionManager");
const interactor = interactionManager.getTargetingInteractor();

script.createEvent("UpdateEvent").bind(() => {
    if (!interactor.isActive()) {
        script.hitTestReticle.enabled = false;
        return;
    }

    // Get ray from interactor
    const rayOrigin = interactor.startPoint;
    const rayDirection = interactor.direction;

    // Perform hit test
    hitTestSession.hitTest(rayOrigin, rayDirection, (results) => {
        if (results && results.length > 0) {
            const hit = results[0];

            // Show reticle at hit point
            script.hitTestReticle.enabled = true;
            script.hitTestReticle.getTransform()
                .setWorldPosition(hit.position);

            // Place object on pinch
            if (interactor.triggerJustReleased()) {
                placeObject(hit.position, hit.normal);
            }
        } else {
            script.hitTestReticle.enabled = false;
        }
    });
});

function placeObject(position: vec3, normal: vec3) {
    // Clone target object
    const clone = script.targetObject.copyWholeHierarchy(
        script.getSceneObject().getParent()
    );

    clone.getTransform().setWorldPosition(position);

    // Orient to surface normal while keeping upright
    const up = vec3.up();
    const forward = normal.cross(up).normalize();
    const rotation = quat.lookAt(forward, up);
    clone.getTransform().setWorldRotation(rotation);

    clone.enabled = true;
}
```

### Video Playback

```typescript
// videoplayback.ts - Video texture control

// @input Asset.Texture videoTexture {"hint":"Video texture provider"}

let videoProvider: VideoTextureProvider;

script.createEvent("OnStartEvent").bind(() => {
    // Get video texture provider
    const control = script.videoTexture.control as VideoTextureProvider;
    if (!control) {
        print("Error: Texture must have VideoTextureProvider");
        return;
    }

    videoProvider = control;

    // Configure video
    videoProvider.loopsCount = 1; // Play once
    videoProvider.volume = 0.8;
    videoProvider.playbackSpeed = 1.0;

    // Start playback
    videoProvider.play();
});

// Public API for controlling video
export function seekVideo(timeInSeconds: number) {
    if (videoProvider) {
        videoProvider.seekTo(timeInSeconds);
    }
}

export function pauseVideo() {
    if (videoProvider && videoProvider.isPlaying()) {
        videoProvider.pause();
    }
}

export function resumeVideo() {
    if (videoProvider && !videoProvider.isPlaying()) {
        videoProvider.resume();
    }
}

export function adjustPlayback(speed: number, volume: number) {
    if (videoProvider) {
        videoProvider.playbackSpeed = speed;
        videoProvider.volume = volume;
    }
}
```

### Body & Hand Tracking

```typescript
// Hand tracking example
// @input Asset.ObjectTracking3D handTrackingAsset

const objectTracking = script.handTrackingAsset;

script.createEvent("OnStartEvent").bind(() => {
    // Listen for tracking events
    objectTracking.onTrackingStarted.add(() => {
        print("Hand tracking started");
    });

    objectTracking.onTrackingLost.add(() => {
        print("Hand tracking lost");
        // Hide hand-dependent UI
    });
});

script.createEvent("UpdateEvent").bind(() => {
    if (!objectTracking.isTracking()) return;

    // Get hand data
    const leftHand = objectTracking.getLeftHand();
    const rightHand = objectTracking.getRightHand();

    if (leftHand) {
        const wrist = leftHand.getJoint("wrist");
        const indexTip = leftHand.getJoint("index_finger_tip");

        // Calculate pinch distance
        const pinchDist = wrist.position.distance(indexTip.position);
        if (pinchDist < 0.02) { // 2cm threshold
            print("Left hand pinching!");
        }
    }
});
```

---

## Example Projects Breakdown

### Project 1: Pulse Map Lens (Location-Based AR)

#### Overview
A location-aware map experience with:
- Custom pins and quest markers
- Snap Places API integration
- Minimap with zoom/pan gestures
- Off-screen directional markers

#### Key Files & Responsibilities

**Core Map System:**
- `MapComponent.ts` - Main entry point, exposes public API
- `MapController.ts` - Workhorse that manages tiles, pins, location service
- `MapGridView.ts` - Tile grid management
- `Cell.ts` - Individual tile cell
- `MapPin.ts` - Pin data model
- `MapUtils.ts` - Utility functions (distance calculation, geometry generation)

**UI Controllers:**
- `MapUIController.ts` - Wires SIK buttons to map actions
- `QuestMarkController.ts` - Off-screen marker system
- `MapContainerController.ts` - Manages ContainerFrame clipping
- `MapMessageController.ts` - Event-driven UI text updates

**API Integration:**
- `SnapPlacesProvider.ts` - Wraps Remote Service Module for Snap Places API
- `Snapchat Places API Module.js` - Remote service configuration

#### Setup Steps

1. Open `pulse.esproj` in Lens Studio
2. Configure Remote Service Module:
   - Go to **Project Info > Remote Services**
   - Ensure Snap Places API module is connected
   - Add your API credentials
3. Enable Location Services capability
4. Pair Spectacles and deploy

#### Key Configuration Inputs

Inspector inputs on `MapComponent` script:

```typescript
// Grid & Tiles
tileCount: number = 3              // 3x3 grid
mapZoomLevel: number = 15          // Zoom level (8-21)
mapRenderParent: SceneObject       // Parent for tile rendering

// User Pin
showUserPin: bool = true
userPinVisual: Asset.ObjectPrefab
userPinScale: vec3 = {1, 1, 1}
userPinAlignedWithOrientation: bool = true

// Map Pins
mapPinPrefab: Asset.ObjectPrefab
mapPinsRotated: bool = false

// Interaction
enableScrolling: bool = true
scrollingFriction: number = 0.9
enableMapSmoothing: bool = true
mapUpdateThreshold: number = 10    // meters

// Custom Location (optional)
setMapToCustomLocation: bool = false
longitude: number = -118.2437
latitude: number = 34.0522
rotation: number = 0

// Advanced
isMinimapAutoRotate: bool = true
mapPinCursorDetectorSize: number = 50
```

#### Key APIs Exposed by MapComponent

```typescript
// Add pin at screen position
addPinByLocalPosition(localPos: vec2): void

// Add pin at geo coordinates
addPinByGeoPosition(lat: number, lon: number): void

// Remove pins
removeMapPins(): void

// Zoom controls
zoomIn(): void
zoomOut(): void

// Center map on user
centerMap(): void

// Show nearby places
showNearbyPlaces(categories: string[]): void

// Events
onMapTilesLoaded: Event
onMapScrolled: Event
onMapPinAdded: Event<MapPin>
onMapPinRemoved: Event<MapPin>
```

#### Extending the Map

**Example: Adding Category-Specific Pin Icons**

```typescript
// Modify MapController.showNearbyPlaces

showNearbyPlaces(categories: string[]) {
    snapPlacesProvider.getNearbyPlaces(currentLat, currentLon, categories)
        .then((places) => {
            places.forEach((place) => {
                // Choose prefab based on category
                let pinPrefab: ObjectPrefab;
                if (place.category.includes("Cafe")) {
                    pinPrefab = this.cafePinPrefab;
                } else if (place.category.includes("Restaurant")) {
                    pinPrefab = this.restaurantPinPrefab;
                } else {
                    pinPrefab = this.defaultPinPrefab;
                }

                // Instantiate with custom prefab
                const pin = this.addPinByGeoPosition(
                    place.latitude,
                    place.longitude,
                    pinPrefab
                );

                // Store place metadata
                pin.metadata = place;
            });
        });
}
```

**Example: Showing Place Details on Pin Pinch**

```typescript
// Add to MapController

setupPinInteraction(pinObject: SceneObject, placeInfo: PlaceInfo) {
    const interactable = pinObject.getComponent("Interactable");

    interactable.onPinch.add(() => {
        // Show info panel
        showPlaceInfoPanel(placeInfo);
    });
}

function showPlaceInfoPanel(place: PlaceInfo) {
    // Update UI text components
    placeNameText.text = place.name;
    placeAddressText.text = place.address;
    placeHoursText.text = place.opening_hours || "Hours not available";

    // Animate panel in
    infoPanel.enabled = true;
}
```

### Project 2: Spectacles Cooking Lens (Voice-Controlled Tutorial)

#### Overview
A voice-driven cooking assistant that:
- Anchors instructional surface in the world
- Plays recipe video synchronized with steps
- Uses voice commands to navigate ("next", "back", "pause")
- Shows live transcription overlay

#### Key Files & Responsibilities

- `worldview.ts` - Surface detection and object placement
- `videoplayback.ts` - Video texture provider abstraction
- `videoPlaybackVoiceController.ts` - Voice command parser and recipe coordinator
- `stt.ts` - Speech-to-text overlay for immediate transcription feedback

#### Setup Steps

1. Open `lahacks25.esproj` in Lens Studio
2. Enable capabilities:
   - Voice ML
   - World Mesh / Surfaces
3. Import recipe videos as Texture assets
4. Configure recipe steps in `videoPlaybackVoiceController.ts`

#### Recipe Data Structure

```typescript
interface CookingStep {
    start_time: number;     // Video timestamp (seconds)
    end_time: number;
    title: string;
    description: string;
}

const beefWellingtonSteps: CookingStep[] = [
    {
        start_time: 0,
        end_time: 15,
        title: "Sear the Beef",
        description: "Heat oil in pan. Sear beef on all sides until browned."
    },
    {
        start_time: 15,
        end_time: 45,
        title: "Prepare Duxelles",
        description: "Finely chop mushrooms. Cook until moisture evaporates."
    },
    // ... more steps
];
```

#### Voice Command Handling

```typescript
// videoPlaybackVoiceController.ts

function handleTranscription(text: string) {
    const command = text.toLowerCase().trim();

    // Next step
    if (command.includes("next") || command.includes("forward")) {
        coordinateNext();
    }
    // Previous step
    else if (command.includes("back") || command.includes("previous")) {
        coordinatePrev();
    }
    // Pause/Resume
    else if (command.includes("pause") || command.includes("stop")) {
        pauseVideo();
        updateMicButton(false);
    }
    else if (command.includes("resume") || command.includes("play")) {
        resumeVideo();
    }
    // Repeat instruction
    else if (command.includes("repeat")) {
        // Seek to start of current step
        seekVideo(currentStep.start_time);
    }
    // Unknown command
    else {
        print("Command not recognized: " + command);
        // Could show visual feedback
    }
}

function coordinateNext() {
    if (currentStepIndex < beefWellingtonSteps.length - 1) {
        currentStepIndex++;
        const step = beefWellingtonSteps[currentStepIndex];

        // Update UI scroll view
        gridContentCreator.scrollToNextInstruction();

        // Seek video to step start
        seekVideo(step.start_time);

        print(`Step ${currentStepIndex + 1}: ${step.title}`);
    }
}

function coordinatePrev() {
    if (currentStepIndex > 0) {
        currentStepIndex--;
        const step = beefWellingtonSteps[currentStepIndex];

        gridContentCreator.scrollToPrevInstruction();
        seekVideo(step.start_time);

        print(`Step ${currentStepIndex + 1}: ${step.title}`);
    }
}
```

#### Extending the Cooking Lens

**Example: Load Recipes from Remote API**

```typescript
// RecipeProvider.ts

export class RecipeProvider {
    // @input Asset.RemoteServiceModule recipeAPI

    loadRecipe(recipeId: string): Promise<CookingStep[]> {
        return new Promise((resolve, reject) => {
            const request = RemoteServiceHttpRequest.create();
            request.url = `https://api.example.com/recipes/${recipeId}`;
            request.method = RemoteServiceHttpRequest.HttpRequestMethod.Get;

            script.recipeAPI.performHttpRequest(request, (response) => {
                if (response.statusCode === 200) {
                    const data = JSON.parse(response.body);
                    resolve(data.steps);
                } else {
                    reject(new Error("Failed to load recipe"));
                }
            });
        });
    }
}

// Usage
const recipeProvider = new RecipeProvider();
recipeProvider.loadRecipe("beef-wellington")
    .then((steps) => {
        beefWellingtonSteps = steps;
        print(`Loaded ${steps.length} steps`);
    });
```

**Example: Add Timer Notifications**

```typescript
let stepTimer: DelayedCallbackEvent;

function coordinateNext() {
    currentStepIndex++;
    const step = beefWellingtonSteps[currentStepIndex];

    // Clear previous timer
    if (stepTimer) {
        script.removeDelayedCallback(stepTimer);
    }

    // Set timer for step duration
    const duration = step.end_time - step.start_time;
    stepTimer = script.createDelayedCallback(() => {
        print("Step completed! Say 'next' to continue.");
        // Show visual notification
    }, duration);

    gridContentCreator.scrollToNextInstruction();
    seekVideo(step.start_time);
}
```

---

## Testing, Debugging & Deployment

### Testing Workflow

#### 1. Desktop Preview
- Use Lens Studio's Preview panel for rapid iteration
- Simulate location: **Preview > Location > Set Custom Location**
- Simulate hand tracking: **Preview > Simulate Hand Tracking**
- View logs: **View > Panels > Logger**

#### 2. Device Testing (Spectacles)

**Pairing:**
1. Ensure Spectacles are on and updated
2. In Lens Studio: **Spectacles > Pair Device**
3. Follow Wi-Fi connection prompts

**Deploying:**
1. Make changes in Lens Studio
2. Click **Spectacles > Send to Device**
3. View real-time updates on glasses
4. Monitor logs in Logger panel

**Best Practices:**
- Test frequently during development
- Check performance (frame rate, memory)
- Verify hand tracking in different lighting conditions
- Test voice commands with background noise

#### 3. Mobile Testing (Snapchat App)

**Via Snapcode:**
1. **File > Publish Lens** (or **Send Lens to Snapchat**)
2. Generate Snapcode
3. Scan with Snapchat app
4. Test on iOS/Android

**Via Camera Kit:**
- For custom mobile apps, integrate Camera Kit SDK
- See: https://developers.snap.com/camera-kit

### Debugging Techniques

#### Logging Best Practices

```typescript
// Logging.ts - Centralized logging utility

export enum LogLevel {
    DEBUG,
    INFO,
    WARN,
    ERROR
}

export class Logger {
    private static level: LogLevel = LogLevel.INFO;

    static setLevel(level: LogLevel) {
        this.level = level;
    }

    static debug(tag: string, message: string) {
        if (this.level <= LogLevel.DEBUG) {
            print(`[DEBUG][${tag}] ${message}`);
        }
    }

    static info(tag: string, message: string) {
        if (this.level <= LogLevel.INFO) {
            print(`[INFO][${tag}] ${message}`);
        }
    }

    static warn(tag: string, message: string) {
        if (this.level <= LogLevel.WARN) {
            print(`[WARN][${tag}] ${message}`);
        }
    }

    static error(tag: string, message: string) {
        if (this.level <= LogLevel.ERROR) {
            print(`[ERROR][${tag}] ${message}`);
        }
    }
}

// Usage
Logger.setLevel(LogLevel.DEBUG);
Logger.debug("MapController", "Initializing map tiles");
Logger.info("SnapPlaces", `Fetched ${places.length} nearby cafes`);
Logger.warn("VideoPlayer", "Video playback stuttering");
Logger.error("API", "Failed to connect to remote service");
```

#### Visual Debugging

```typescript
// Draw debug geometry
function debugDrawRay(origin: vec3, direction: vec3, color: vec4) {
    const debugLine = scene.createSceneObject("DebugLine");
    const lineRender = debugLine.createComponent("Component.RenderMeshVisual");

    // Create line mesh
    const mesh = createLineMesh(origin, origin.add(direction.uniformScale(100)));
    lineRender.mesh = mesh;

    // Set material color
    const material = lineRender.getMaterial(0);
    material.mainPass.baseColor = color;
}

// Draw debug sphere at position
function debugDrawSphere(position: vec3, radius: number, color: vec4) {
    const debugSphere = global.scene.createSceneObject("DebugSphere");
    const sphereRender = debugSphere.createComponent("Component.RenderMeshVisual");

    sphereRender.mesh = createSphereMesh(radius);
    debugSphere.getTransform().setWorldPosition(position);

    const material = sphereRender.getMaterial(0);
    material.mainPass.baseColor = color;
}
```

#### Performance Profiling

```typescript
// Measure execution time
class PerformanceTimer {
    private startTime: number;

    start() {
        this.startTime = getTime();
    }

    end(label: string) {
        const elapsed = getTime() - this.startTime;
        Logger.info("Performance", `${label}: ${(elapsed * 1000).toFixed(2)}ms`);
    }
}

// Usage
const timer = new PerformanceTimer();
timer.start();
performExpensiveOperation();
timer.end("Expensive Operation");
```

**Key Metrics to Monitor:**
- Draw calls: Keep under 200 for Spectacles
- Texture memory: Target <100MB total
- Script execution time: Aim for <5ms per frame
- FPS: Maintain 60 FPS on Spectacles

### Deployment Checklist

#### Pre-Submission Validation

1. **Run Project Validation**
   - **Project Info > Validate Project**
   - Resolve all errors and warnings
   - Fix missing inputs, null references

2. **Test on Target Devices**
   - Verify on Spectacles hardware
   - Test on multiple iOS/Android devices
   - Check different lighting conditions

3. **Optimize Performance**
   - Remove unused assets
   - Compress textures
   - Reduce draw calls
   - Profile frame time

4. **Verify Permissions**
   - Location Services (if used)
   - Microphone (for Voice ML)
   - Camera access
   - Network access (for Remote Services)

5. **Documentation**
   - User instructions (how to use the lens)
   - Privacy policy (if collecting data)
   - Third-party licenses

#### Publishing to Snap

1. **Prepare Assets**
   - Lens icon (1024x1024 PNG)
   - Preview video (vertical format, 9:16)
   - Screenshots

2. **Metadata**
   - Lens name
   - Description (clear, concise)
   - Category (AR Games, Utilities, Education, etc.)
   - Tags/Keywords

3. **Submit for Review**
   - **File > Publish Lens**
   - Fill out submission form
   - Provide testing instructions for reviewers
   - Mention any special requirements (location, etc.)

4. **Review Process**
   - Typically 1-3 business days
   - Check email for reviewer feedback
   - Address any issues and resubmit

5. **Post-Launch**
   - Monitor analytics (views, playtime, shares)
   - Collect user feedback
   - Plan updates/improvements

#### Release Management

**Maintain RELEASE.md:**

```markdown
# Release History

## v1.2.0 - 2025-01-15
### Added
- Voice command support for recipe navigation
- Real-time transcription overlay

### Fixed
- Video playback stuttering on certain devices
- Hand tracking loss recovery

### Performance
- Reduced texture memory by 30%
- Optimized tile loading

## v1.1.0 - 2024-12-20
### Added
- Snap Places integration for nearby cafes
- Custom pin icons by category

### Changed
- Improved map pan gesture smoothness

## v1.0.0 - 2024-11-30
- Initial release
```

---

## API Reference Quick Links

### Official Snap Documentation

- **Spectacles Home**: https://developers.snap.com/spectacles/home
- **Lens Studio Overview**: https://developers.snap.com/lens-studio/overview/getting-started/lens-studio-overview
- **Spectacles Getting Started**: https://developers.snap.com/spectacles/get-started/introduction
- **Build Your First Lens**: https://developers.snap.com/spectacles/get-started/start-building/build-your-first-spectacles-lens-tutorial

### Lens Scripting API

- **Full API Documentation**: https://developers.snap.com/lens-studio/api/lens-scripting/index.html
- **SceneObject**: https://developers.snap.com/lens-studio/api/lens-scripting/classes/Built-In.SceneObject.html
- **Camera**: https://developers.snap.com/lens-studio/api/lens-scripting/classes/Built-In.Camera.html
- **CameraModule**: https://developers.snap.com/lens-studio/api/lens-scripting/classes/Built-In.CameraModule.html
- **Components**: https://developers.snap.com/api/lens-studio/Classes/Components
- **Script Objects**: https://developers.snap.com/api/lens-studio/Classes/ScriptObjects

### Spectacles Interaction Kit (SIK)

- **SIK Getting Started**: https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/get-started
- **Interaction System**: https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/features/interactionsystem
- **Hand Tracking**: https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/features/handtracking
- **Mobile Controller**: https://developers.snap.com/spectacles/spectacles-frameworks/spectacles-interaction-kit/features/mobilecontroller

### TypeScript Development

- **TypeScript Guide**: https://developers.snap.com/lens-studio/features/scripting/typescript
- **Editor Scripting API**: https://developers.snap.com/lens-studio/api/editor-scripting/

### Downloads

- **Lens Studio**: https://ar.snap.com/download

---

## Troubleshooting Guide

### Map Lens Issues

#### Problem: Map Tiles Not Loading

**Symptoms:**
- Blank map area
- No tiles visible after location acquired

**Solutions:**
1. Verify `MapModule` reference is set in `Map Controller.prefab`
2. Check device has network access
3. Ensure tile prefab has correct `ScreenTransform` setup
4. Check Logger for tile loading errors
5. Verify `tileCount` and `mapZoomLevel` are valid values

```typescript
// Add logging to diagnose
Logger.debug("MapController", `Loading tiles: zoom=${zoomLevel}, count=${tileCount}`);
```

#### Problem: User Pin Not Showing

**Symptoms:**
- User location detected but pin invisible

**Solutions:**
1. Verify `showUserPin` is enabled in Inspector
2. Check `userPinVisual` points to valid prefab
3. Ensure prefab has `ScreenTransform` component
4. Verify pin scale is reasonable (`userPinScale`)
5. Check if pin is rendering behind tiles (adjust render order)

#### Problem: Nearby Places API Returns Empty

**Symptoms:**
- No pins appear when calling `showNearbyPlaces`

**Solutions:**
1. Verify Remote Service Module configured with valid credentials
2. Check category filter strings match API categories
3. Test with `null` filter to fetch all venues
4. Check network connectivity
5. Review Logger for API error responses

```typescript
// Debug API call
snapPlacesProvider.getNearbyPlaces(lat, lon, ["Cafe"])
    .then((places) => {
        Logger.info("API", `Received ${places.length} places`);
    })
    .catch((error) => {
        Logger.error("API", `Failed: ${error.message}`);
    });
```

#### Problem: Quest Markers Overlap

**Symptoms:**
- Multiple off-screen markers stack on top of each other

**Solutions:**
1. Verify `UICollisionSolver` is enabled in `QuestMarkController`
2. Adjust marker spacing constants
3. Implement marker prioritization (hide lower-priority markers)

```typescript
// Adjust collision solver settings
const solver = new UICollisionSolver();
solver.minSpacing = 50; // Pixels between markers
solver.resolve(markers);
```

### Voice & Video Lens Issues

#### Problem: Voice Commands Not Recognized

**Symptoms:**
- Speech detected but commands ignored
- Mic button doesn't show "listening" state

**Solutions:**
1. Check VoiceMLModule is enabled in **Capabilities**
2. Verify `normalMicImage` and `listeningMicImage` are assigned
3. Add logging to `handleTranscription` to see received text
4. Test with clearer enunciation and less background noise
5. Add more command variations (synonyms)

```typescript
function handleTranscription(text: string) {
    Logger.debug("Voice", `Received: "${text}"`);

    const command = text.toLowerCase().trim();

    // Add variations
    if (command.includes("next") ||
        command.includes("forward") ||
        command.includes("continue") ||
        command.includes("skip")) {
        coordinateNext();
    }
}
```

#### Problem: Video Doesn't Play

**Symptoms:**
- Video texture shows black screen
- No audio

**Solutions:**
1. Verify texture has `VideoTextureProvider` control
2. Check video format is supported (MP4, H.264)
3. Ensure video file is imported correctly (check Resources panel)
4. Check `loopsCount` is > 0
5. Verify volume is > 0

```typescript
// Diagnostic logging
Logger.debug("Video", `Provider: ${videoProvider ? 'OK' : 'NULL'}`);
Logger.debug("Video", `Playing: ${videoProvider.isPlaying()}`);
Logger.debug("Video", `Current time: ${videoProvider.getCurrentTime()}`);
```

#### Problem: Hit Test Fails (World Anchoring)

**Symptoms:**
- Reticle never appears
- Objects won't place

**Solutions:**
1. Verify **Surfaces** capability is enabled
2. Ensure testing on hardware with world mesh support
3. Check `WorldQueryModule` is imported correctly
4. Test in well-lit environment with textured surfaces
5. Verify interactor ray direction is correct

```typescript
// Debug ray
Logger.debug("HitTest", `Ray: origin=${rayOrigin}, dir=${rayDirection}`);

// Visualize ray
debugDrawRay(rayOrigin, rayDirection, new vec4(1, 0, 0, 1));
```

### Performance Issues

#### Problem: Low Frame Rate

**Symptoms:**
- Choppy visuals
- Delayed interactions

**Solutions:**
1. Reduce draw calls:
   - Combine meshes
   - Use texture atlases
   - Reduce prefab count
2. Optimize scripts:
   - Move expensive calculations out of `UpdateEvent`
   - Cache results instead of recalculating
   - Use object pooling for frequently created/destroyed objects
3. Compress textures:
   - Enable compression in texture import settings
   - Reduce resolution where possible
4. Profile with Lens Studio's performance tools

```typescript
// Object pooling example
class ObjectPool {
    private pool: SceneObject[] = [];

    acquire(): SceneObject {
        if (this.pool.length > 0) {
            const obj = this.pool.pop()!;
            obj.enabled = true;
            return obj;
        } else {
            return createNewObject();
        }
    }

    release(obj: SceneObject) {
        obj.enabled = false;
        this.pool.push(obj);
    }
}
```

#### Problem: High Memory Usage

**Symptoms:**
- Lens crashes on device
- Textures fail to load

**Solutions:**
1. Reduce texture sizes
2. Enable texture compression
3. Remove unused assets from project
4. Use texture streaming for large textures
5. Implement asset unloading when not needed

```typescript
// Unload texture when not visible
function hideObject(obj: SceneObject) {
    obj.enabled = false;

    // Optional: unload texture
    const image = obj.getComponent("Component.Image");
    if (image) {
        image.mainPass.baseTex = null; // Release texture memory
    }
}
```

### Build & Deployment Issues

#### Problem: Validation Errors

**Symptoms:**
- Project validation shows errors
- Can't publish

**Solutions:**
1. Fix missing script inputs (red highlighting in Inspector)
2. Verify all prefab references are valid
3. Remove broken component references
4. Check for null assets
5. Resolve naming conflicts

#### Problem: Lens Rejected in Review

**Common Reasons:**
- Performance issues (low FPS, crashes)
- Unclear user instructions
- Privacy violations (unauthorized data collection)
- Inappropriate content

**Solutions:**
1. Address specific feedback from reviewers
2. Add clear instructions (UI hints, tutorial)
3. Remove any data collection or explain in privacy policy
4. Test thoroughly before resubmission

---

## Advanced Topics & Extensions

### Building a Hands-Free Auto-Reorder System

Based on your project prompt, here's an architecture for building an auto-reorder system using Spectacles:

#### System Architecture

```
┌─────────────────────────────────────────────────────┐
│ Spectacles (On-Device)                              │
├─────────────────────────────────────────────────────┤
│ 1. Video Capture                                    │
│    └─> Camera feed (real-time POV)                 │
│                                                     │
│ 2. On-Device CV (Object Detection + State)         │
│    └─> TensorFlow Lite / ONNX model                │
│    └─> Detects: [Water Bottle, Sunscreen, etc.]   │
│    └─> Classifies: [Full, Half, Empty]            │
│                                                     │
│ 3. Trigger Detection                                │
│    └─> If state == "Empty" → Show prompt           │
│                                                     │
│ 4. User Confirmation                                │
│    └─> Hand gesture (pinch) or voice ("Yes")       │
│                                                     │
│ 5. Send Order Request                               │
│    └─> Remote Service Module → Backend API         │
└─────────────────────────────────────────────────────┘
           │
           ▼
┌─────────────────────────────────────────────────────┐
│ Backend Server                                       │
├─────────────────────────────────────────────────────┤
│ 1. Receive order request                            │
│ 2. Lookup product from object class                 │
│ 3. Call Commerce API (Amazon, Walmart, Instacart)  │
│ 4. Confirm order placement                          │
│ 5. Send confirmation to Spectacles                  │
└─────────────────────────────────────────────────────┘
```

#### Step 1: On-Device Computer Vision

**Option A: ML Component (Lens Studio Built-In)**

Lens Studio supports ML models via the **ML Component**:

1. Train a custom object detection model (TensorFlow Lite or PyTorch → ONNX)
2. Export as `.onnx` or `.tflite`
3. Import into Lens Studio as **ML Component**

```typescript
// @input Asset.MLAsset mlModel
// @input Component.Camera camera

const mlComponent = script.getSceneObject().createComponent("Component.MLComponent");
mlComponent.model = script.mlModel;

script.createEvent("UpdateEvent").bind(() => {
    // Run inference on camera frame
    const input = mlComponent.build([camera.renderTarget]);
    mlComponent.runImmediate(input);

    // Get output tensor
    const output = mlComponent.getOutput("detection_boxes");

    // Parse detections
    parseDetections(output);
});

function parseDetections(tensor: Tensor) {
    // tensor contains bounding boxes and class scores
    // Example format: [x, y, width, height, class_id, confidence]

    for (let i = 0; i < tensor.shape[0]; i++) {
        const classId = tensor.data[i * 6 + 4];
        const confidence = tensor.data[i * 6 + 5];

        if (confidence > 0.7) {
            const objectClass = getObjectClassName(classId);
            const emptyState = checkIfEmpty(tensor.data, i);

            if (emptyState) {
                showReorderPrompt(objectClass);
            }
        }
    }
}

function checkIfEmpty(detectionData: number[], index: number): boolean {
    // Analyze bounding box fill level
    // Could use secondary model or heuristics
    // E.g., check if water line is visible in bottle

    // Placeholder logic
    return Math.random() > 0.7; // TODO: Replace with actual CV logic
}
```

**Option B: Remote Inference**

For more complex models:

1. Stream video frames to backend
2. Run inference on cloud GPU
3. Receive detection results

```typescript
// @input Asset.RemoteServiceModule cvAPI

function sendFrameForInference(frame: Texture) {
    const base64Frame = textureToBase64(frame);

    const request = RemoteServiceHttpRequest.create();
    request.url = "https://api.yourbackend.com/detect";
    request.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;
    request.body = JSON.stringify({ frame: base64Frame });

    script.cvAPI.performHttpRequest(request, (response) => {
        if (response.statusCode === 200) {
            const result = JSON.parse(response.body);
            handleDetections(result.detections);
        }
    });
}
```

#### Step 2: Non-Invasive Prompt UI

```typescript
// @input SceneObject promptPanel
// @input Component.Text promptText
// @input SceneObject confirmButton
// @input SceneObject dismissButton

let currentProduct: string | null = null;

function showReorderPrompt(productName: string) {
    currentProduct = productName;

    // Update prompt text
    script.promptText.text = `${productName} is running low.\nReorder now?`;

    // Animate panel in (subtle, non-intrusive)
    script.promptPanel.enabled = true;
    animatePromptIn();

    // Auto-dismiss after 10 seconds if no response
    script.createDelayedCallback(() => {
        if (currentProduct === productName) {
            dismissPrompt();
        }
    }, 10.0);
}

function animatePromptIn() {
    // Fade in animation
    const startAlpha = 0;
    const endAlpha = 1;
    const duration = 0.5;

    animateAlpha(script.promptPanel, startAlpha, endAlpha, duration);
}

function dismissPrompt() {
    animateAlpha(script.promptPanel, 1, 0, 0.3, () => {
        script.promptPanel.enabled = false;
        currentProduct = null;
    });
}
```

#### Step 3: Gesture & Voice Confirmation

```typescript
// Gesture confirmation (pinch on confirm button)
const confirmInteractable = script.confirmButton.getComponent("Interactable");
confirmInteractable.onPinch.add(() => {
    if (currentProduct) {
        placeOrder(currentProduct);
        dismissPrompt();
    }
});

// Voice confirmation
const voiceML = script.getSceneObject().getComponent("VoiceMLModule");

voiceML.onListeningUpdate.add((eventArgs) => {
    if (eventArgs.isFinalTranscription) {
        const text = eventArgs.transcription.toLowerCase().trim();

        if (text.includes("yes") || text.includes("order") || text.includes("reorder")) {
            if (currentProduct) {
                placeOrder(currentProduct);
                dismissPrompt();
            }
        } else if (text.includes("no") || text.includes("dismiss") || text.includes("cancel")) {
            dismissPrompt();
        }
    }
});
```

#### Step 4: Commerce API Integration

```typescript
// @input Asset.RemoteServiceModule commerceAPI

function placeOrder(productName: string) {
    Logger.info("Order", `Placing order for: ${productName}`);

    const request = RemoteServiceHttpRequest.create();
    request.url = "https://api.yourbackend.com/order";
    request.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;
    request.body = JSON.stringify({
        product: productName,
        quantity: 1,
        user_id: getUserId() // From Snap account or stored preference
    });

    script.commerceAPI.performHttpRequest(request, (response) => {
        if (response.statusCode === 200) {
            const result = JSON.parse(response.body);
            showOrderConfirmation(result.order_id);
        } else {
            showOrderError();
        }
    });
}

function showOrderConfirmation(orderId: string) {
    // Show success message
    script.promptText.text = `Order placed! ID: ${orderId}`;

    // Auto-dismiss after 3 seconds
    script.createDelayedCallback(() => {
        dismissPrompt();
    }, 3.0);
}

function showOrderError() {
    script.promptText.text = "Order failed. Please try again.";
}
```

#### Step 5: Privacy & On-Device Processing

To maximize privacy and reduce latency:

1. **Run CV model on-device** (Lens Studio ML Component)
2. **Only send product name** to backend (not video frames)
3. **Cache user preferences** locally
4. **Encrypt API requests** (HTTPS)

```typescript
// Local product database (no server needed for lookup)
const productDatabase = {
    "water_bottle": {
        name: "Spring Water 24-pack",
        asin: "B001234567", // Amazon ASIN
        price: 12.99
    },
    "sunscreen": {
        name: "SPF 50 Sunscreen",
        asin: "B007654321",
        price: 15.49
    }
};

function placeOrder(productClass: string) {
    const product = productDatabase[productClass];

    if (!product) {
        Logger.error("Order", `Unknown product: ${productClass}`);
        return;
    }

    // Only send product identifier, not video data
    const request = RemoteServiceHttpRequest.create();
    request.url = "https://api.yourbackend.com/order";
    request.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;
    request.body = JSON.stringify({
        asin: product.asin,
        quantity: 1
    });

    script.commerceAPI.performHttpRequest(request, handleOrderResponse);
}
```

### Multiplayer & Connected Experiences

For shared AR experiences (multiple users seeing same content):

```typescript
// @input Asset.ConnectedLensModule connectedLens

const session = script.connectedLens;

// Join session
session.createSession();

// Send data to other users
function shareObjectPlacement(position: vec3, objectType: string) {
    const message = {
        type: "object_placed",
        position: { x: position.x, y: position.y, z: position.z },
        objectType: objectType
    };

    session.sendMessage(JSON.stringify(message));
}

// Receive data from other users
session.onMessageReceived.add((userId, message) => {
    const data = JSON.parse(message);

    if (data.type === "object_placed") {
        const pos = new vec3(data.position.x, data.position.y, data.position.z);
        spawnObject(data.objectType, pos);
    }
});

// Handle user join/leave
session.onUserJoinedSession.add((userId) => {
    Logger.info("Multiplayer", `User ${userId} joined`);
});

session.onUserLeftSession.add((userId) => {
    Logger.info("Multiplayer", `User ${userId} left`);
});
```

### Persistence & Cloud Storage

To save user data across sessions:

```typescript
// @input Asset.PersistentStorageSystem storage

// Save data locally
function saveUserPreferences(preferences: any) {
    const store = script.storage.store;
    store.set("user_preferences", JSON.stringify(preferences));
}

// Load data
function loadUserPreferences(): any {
    const store = script.storage.store;
    const data = store.get("user_preferences");
    return data ? JSON.parse(data) : {};
}

// For cloud sync, use Remote Service Module
function syncToCloud(data: any) {
    const request = RemoteServiceHttpRequest.create();
    request.url = "https://api.yourbackend.com/sync";
    request.method = RemoteServiceHttpRequest.HttpRequestMethod.Post;
    request.body = JSON.stringify(data);

    script.remoteAPI.performHttpRequest(request, (response) => {
        Logger.info("Sync", "Data synced to cloud");
    });
}
```

---

## Final Thoughts & Best Practices Summary

### Development Mantras

1. **Start simple, iterate quickly**
   - Build minimal viable lens first
   - Add features incrementally
   - Test on device frequently

2. **DRY (Don't Repeat Yourself)**
   - Extract shared logic into utilities
   - Use prefabs for reusable components
   - Reference documentation instead of duplicating

3. **YAGNI (You Aren't Gonna Need It)**
   - Don't build features you *might* need
   - Focus on core requirements
   - Avoid over-engineering

4. **SOLID Principles**
   - Single Responsibility: One purpose per script
   - Open/Closed: Extend, don't modify
   - Dependency Injection: Use `@input` annotations

5. **Document everything**
   - Script headers with purpose, inputs, outputs
   - Inline comments for complex logic
   - Companion docs for state machines
   - Update docs with every feature change

### Quick Reference Cheat Sheet

#### Common Script Patterns

```typescript
// Script template
/// <reference path="../Support/StudioLib.d.ts" />

// @input SceneObject myObject
// @input Component.Camera camera

script.createEvent("OnStartEvent").bind(onStart);
script.createEvent("UpdateEvent").bind(onUpdate);

function onStart() {
    // Initialize
}

function onUpdate() {
    const dt = getDeltaTime();
    // Update logic
}
```

#### Essential APIs

```typescript
// Scene hierarchy
const obj = script.getSceneObject();
const parent = obj.getParent();
const child = obj.getChild(0);

// Transform
const transform = obj.getTransform();
transform.setWorldPosition(new vec3(0, 0, -50));
transform.setWorldRotation(quat.fromEulerAngles(0, Math.PI, 0));

// Components
const camera = obj.getComponent("Component.Camera") as Camera;
const text = obj.getComponent("Component.Text") as Text;
text.text = "Hello, Spectacles!";

// Events
const event = script.createEvent("UpdateEvent");
event.bind(() => { /* logic */ });

// Delayed callbacks
script.createDelayedCallback(() => {
    print("Delayed action");
}, 2.0); // 2 seconds
```

#### SIK Patterns

```typescript
// Get interactor
const interactor = SIK.InteractionManager.getTargetingInteractor();

// Pinch button
const button = obj.getComponent("PinchButton");
button.onPinch.add(() => { /* action */ });

// Voice ML
const voiceML = obj.getComponent("VoiceMLModule");
voiceML.onListeningUpdate.add((eventArgs) => {
    if (eventArgs.isFinalTranscription) {
        handleCommand(eventArgs.transcription);
    }
});
```

### Next Steps

1. **Download Lens Studio** and create your first Spectacles lens
2. **Clone the starter project** (Spectacles Base Template)
3. **Explore SIK prefabs** and customize UI
4. **Build a simple interaction** (e.g., place object with hand gesture)
5. **Add voice commands** for navigation
6. **Integrate location** if building mapping features
7. **Deploy to Spectacles** and test in real environment
8. **Iterate based on feedback**

### Resources Recap

- **Snap Developers Portal**: https://developers.snap.com
- **Lens Studio Download**: https://ar.snap.com/download
- **Spectacles Docs**: https://developers.snap.com/spectacles/home
- **API Reference**: https://developers.snap.com/lens-studio/api/lens-scripting/
- **Community Forum**: https://support.lensstudio.snapchat.com

---

**Happy building! May your lenses be bug-free and your frame rates be high. 🐾**

*Generated by Claude for Pierce – Your friendly AI coding companion*
